{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edb82653-7f74-4f7f-a741-5485b3d98fc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n  Obtaining dependency information for python-dotenv from https://files.pythonhosted.org/packages/1e/18/98a99ad95133c6a6e2005fe89faedf294a748bd5dc803008059409ac9b1e/python_dotenv-1.1.0-py3-none-any.whl.metadata\n  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\nDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\nInstalling collected packages: python-dotenv\nSuccessfully installed python-dotenv-1.1.0\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d107acdf-aca8-4b53-b345-ea6db9527ea1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "from databricks import feature_engineering\n",
    "from databricks.feature_engineering import FeatureLookup\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from lightgbm import LGBMClassifier\n",
    "from mlflow.models import infer_signature\n",
    "# from pyspark.sql import SparkSession\n",
    "from databricks.connect import DatabricksSession\n",
    "from pyspark.sql import functions as F\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from utils import load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96c16da8-bb22-44d4-a5ae-7b2a02ceadab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-03-28 20:55:08.039\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36mutils\u001B[0m:\u001B[36mload_config\u001B[0m:\u001B[36m66\u001B[0m - \u001B[1mLoaded configuration from ../project_config.yml\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "catalog_name='test01' schema_name='default' pipeline_id='4be66e88-11e8-4988-9fa3-459b9b28a83c' parameters={'learning_rate': 0.05, 'random_state': 42, 'force_col_wise': True} ab_test={'learning_rate_a': 0.05, 'learning_rate_b': 0.1, 'force_col_wise': True} num_features=[NumFeature(name='ID', dtype='int64'), NumFeature(name='LIMIT_BAL', dtype='float64'), NumFeature(name='SEX', dtype='int64'), NumFeature(name='EDUCATION', dtype='int64'), NumFeature(name='MARRIAGE', dtype='int64'), NumFeature(name='AGE', dtype='int64'), NumFeature(name='PAY_0', dtype='int64'), NumFeature(name='PAY_2', dtype='int64'), NumFeature(name='PAY_3', dtype='int64'), NumFeature(name='PAY_4', dtype='int64'), NumFeature(name='PAY_5', dtype='int64'), NumFeature(name='PAY_6', dtype='int64'), NumFeature(name='BILL_AMT1', dtype='float64'), NumFeature(name='BILL_AMT2', dtype='float64'), NumFeature(name='BILL_AMT3', dtype='float64'), NumFeature(name='BILL_AMT4', dtype='float64'), NumFeature(name='BILL_AMT5', dtype='float64'), NumFeature(name='BILL_AMT6', dtype='float64'), NumFeature(name='PAY_AMT1', dtype='float64'), NumFeature(name='PAY_AMT2', dtype='float64'), NumFeature(name='PAY_AMT3', dtype='float64'), NumFeature(name='PAY_AMT4', dtype='float64'), NumFeature(name='PAY_AMT5', dtype='float64'), NumFeature(name='PAY_AMT6', dtype='float64')] target=[Target(name='default.payment.next.month', dtype='int64', new_name='Default')] features=Features(clean=['Id', 'Limit_bal', 'Sex', 'Education', 'Marriage', 'Age', 'Pay_0', 'Pay_2', 'Pay_3', 'Pay_4', 'Pay_5', 'Pay_6', 'Bill_amt1', 'Bill_amt2', 'Bill_amt3', 'Bill_amt4', 'Bill_amt5', 'Bill_amt6', 'Pay_amt1', 'Pay_amt2', 'Pay_amt3', 'Pay_amt4', 'Pay_amt5', 'Pay_amt6'], robust=['Limit_bal', 'Bill_amt1', 'Bill_amt2', 'Bill_amt3', 'Bill_amt4', 'Bill_amt5', 'Bill_amt6', 'Pay_amt1', 'Pay_amt2', 'Pay_amt3', 'Pay_amt4', 'Pay_amt5', 'Pay_amt6'])\n"
     ]
    }
   ],
   "source": [
    "config = load_config(\"../project_config.yml\")\n",
    "parameters = config.parameters\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c2ef44b-0a2f-498e-a7b0-4ff0922206c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Spark and feature engineering client\n",
    "spark = DatabricksSession.builder.getOrCreate()\n",
    "fe = feature_engineering.FeatureEngineeringClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bceab96-be82-4c98-8884-dc16ecd7df9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [\n",
    "    \"Limit_bal\",\n",
    "    \"Sex\",\n",
    "    \"Education\",\n",
    "    \"Marriage\",\n",
    "    \"Age\",\n",
    "    \"Pay_0\",\n",
    "    \"Pay_2\",\n",
    "    \"Pay_3\",\n",
    "    \"Pay_4\",\n",
    "    \"Pay_5\",\n",
    "    \"Pay_6\",\n",
    "    \"Bill_amt1\",\n",
    "    \"Bill_amt2\",\n",
    "    \"Bill_amt3\",\n",
    "    \"Bill_amt4\",\n",
    "    \"Bill_amt5\",\n",
    "    \"Bill_amt6\",\n",
    "    \"Pay_amt1\",\n",
    "    \"Pay_amt2\",\n",
    "    \"Pay_amt3\",\n",
    "    \"Pay_amt4\",\n",
    "    \"Pay_amt5\",\n",
    "    \"Pay_amt6\",\n",
    "]\n",
    "\n",
    "# First, create the feature table with original data\n",
    "create_table_sql = f\"\"\"\n",
    "CREATE OR REPLACE TABLE {config.catalog_name}.{config.schema_name}.features_balanced\n",
    "(Id STRING NOT NULL,\n",
    " {', '.join([f'{col} DOUBLE' for col in columns])})\n",
    "\"\"\"\n",
    "spark.sql(create_table_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfc77550-4e97-4ae4-a42f-ce8695357345",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add primary key and enable CDF\n",
    "spark.sql(\n",
    "    f\"ALTER TABLE {config.catalog_name}.{config.schema_name}.features_balanced ADD CONSTRAINT features_balanced_pk PRIMARY KEY(Id);\"\n",
    ")\n",
    "spark.sql(\n",
    "    f\"ALTER TABLE {config.catalog_name}.{config.schema_name}.features_balanced SET TBLPROPERTIES (delta.enableChangeDataFeed = true);\"\n",
    ")\n",
    "# Convert Spark DataFrame to Pandas for SMOTE\n",
    "train_pdf = spark.table(f\"{config.catalog_name}.{config.schema_name}.train_set\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fdcc249-25ea-466a-bf77-17d21bd1a879",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 24000 entries, 0 to 23999\nData columns (total 26 columns):\n #   Column                Non-Null Count  Dtype         \n---  ------                --------------  -----         \n 0   Id                    24000 non-null  object        \n 1   Limit_bal             24000 non-null  float64       \n 2   Sex                   24000 non-null  int32         \n 3   Education             24000 non-null  int32         \n 4   Marriage              24000 non-null  int32         \n 5   Age                   24000 non-null  int32         \n 6   Pay_0                 24000 non-null  int32         \n 7   Pay_2                 24000 non-null  int32         \n 8   Pay_3                 24000 non-null  int32         \n 9   Pay_4                 24000 non-null  int32         \n 10  Pay_5                 24000 non-null  int32         \n 11  Pay_6                 24000 non-null  int32         \n 12  Bill_amt1             24000 non-null  float64       \n 13  Bill_amt2             24000 non-null  float64       \n 14  Bill_amt3             24000 non-null  float64       \n 15  Bill_amt4             24000 non-null  float64       \n 16  Bill_amt5             24000 non-null  float64       \n 17  Bill_amt6             24000 non-null  float64       \n 18  Pay_amt1              24000 non-null  float64       \n 19  Pay_amt2              24000 non-null  float64       \n 20  Pay_amt3              24000 non-null  float64       \n 21  Pay_amt4              24000 non-null  float64       \n 22  Pay_amt5              24000 non-null  float64       \n 23  Pay_amt6              24000 non-null  float64       \n 24  Default               24000 non-null  int32         \n 25  Update_timestamp_utc  24000 non-null  datetime64[ns]\ndtypes: datetime64[ns](1), float64(13), int32(11), object(1)\nmemory usage: 3.8+ MB\n"
     ]
    }
   ],
   "source": [
    "train_pdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "017c52e8-1bdc-43e4-adaf-8a6d4d3edf11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = train_pdf[columns]\n",
    "y = train_pdf[\"Default\"]\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_balanced, y_balanced = smote.fit_resample(X, y)\n",
    "\n",
    "# Create balanced DataFrame using only the train_set\n",
    "balanced_df = pd.DataFrame(X_balanced, columns=columns)\n",
    "\n",
    "# Identify the number of original samples\n",
    "num_original_samples = len(train_pdf)\n",
    "\n",
    "# Retain original Ids for the real samples and create new Ids for synthetic samples\n",
    "# Start with 30001 to avoid conflicts with existing Ids\n",
    "balanced_df[\"Id\"] = train_pdf[\"Id\"].values.tolist() + [\n",
    "    str(i) for i in range(30001, 30001 + len(balanced_df) - num_original_samples)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84f5991d-df6a-4689-a73a-4f9504ef1042",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "37354"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(balanced_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cbb0662-16bd-49a7-b06f-8e13458c6631",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert back to Spark DataFrame and insert into feature table\n",
    "balanced_spark_df = spark.createDataFrame(balanced_df)\n",
    "\n",
    "# Cast columns in balanced_spark_df to match the schema of the Delta table\n",
    "columns_to_cast = [\"Sex\", \"Education\", \"Marriage\", \"Age\", \"Pay_0\", \"Pay_2\", \"Pay_3\", \"Pay_4\", \"Pay_5\", \"Pay_6\"]\n",
    "\n",
    "for column in columns_to_cast:\n",
    "    balanced_spark_df = balanced_spark_df.withColumn(column, F.col(column).cast(\"double\"))\n",
    "\n",
    "balanced_spark_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\n",
    "    f\"{config.catalog_name}.{config.schema_name}.features_balanced\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "920ff4d6-2dfa-4d18-9db7-691ce1f17a9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The table has 37354 rows.\n"
     ]
    }
   ],
   "source": [
    "# Execute SQL to count rows\n",
    "row_count = spark.sql(\n",
    "    f\"SELECT COUNT(*) AS row_count FROM {config.catalog_name}.{config.schema_name}.features_balanced\"\n",
    ").collect()[0][\"row_count\"]\n",
    "print(f\"The table has {row_count} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa911f15-b83b-4e26-a3c5-da25a24c9397",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate IDs found.\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicates in the 'Id' column\n",
    "duplicate_ids = balanced_df[balanced_df[\"Id\"].duplicated()]\n",
    "\n",
    "if duplicate_ids.empty:\n",
    "    print(\"No duplicate IDs found.\")\n",
    "else:\n",
    "    print(f\"Duplicate IDs found:\\n{duplicate_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf3e2e83-fecc-4790-bbb0-43d21314a09c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Now use create_training_set to create balanced training set\n",
    "# Drop the original features that will be looked up from the feature store\n",
    "# Define the list of columns you want to drop, including \"Update_timestamp_utc\"\n",
    "columns_to_drop = columns + [\"Update_timestamp_utc\"]\n",
    "\n",
    "# Drop the specified columns from the train_set\n",
    "train_set = spark.table(f\"{config.catalog_name}.{config.schema_name}.train_set\").drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e87cec9-e6f2-43ec-bb71-6b691ab3b015",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"databricks\")\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "training_set = fe.create_training_set(\n",
    "    df=train_set,\n",
    "    label=\"Default\",\n",
    "    feature_lookups=[\n",
    "        FeatureLookup(\n",
    "            table_name=f\"{config.catalog_name}.{config.schema_name}.features_balanced\",\n",
    "            feature_names=columns,\n",
    "            lookup_key=\"Id\",\n",
    "        )\n",
    "    ],\n",
    "    exclude_columns=[\"Update_timestamp_utc\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ddbf8ab-5a5f-45f5-a0e3-04210a90d4c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load feature-engineered DataFrame\n",
    "training_df = training_set.load_df().toPandas()\n",
    "test_set = spark.table(f\"{config.catalog_name}.{config.schema_name}.test_set\").toPandas()\n",
    "\n",
    "# Split features and target (exclude 'Id' from features)\n",
    "X_train = training_df[columns]\n",
    "y_train = training_df[\"Default\"]\n",
    "X_test = test_set[columns]\n",
    "y_test = test_set[\"Default\"]\n",
    "\n",
    "features_robust = [\n",
    "    \"Limit_bal\",\n",
    "    \"Bill_amt1\",\n",
    "    \"Bill_amt2\",\n",
    "    \"Bill_amt3\",\n",
    "    \"Bill_amt4\",\n",
    "    \"Bill_amt5\",\n",
    "    \"Bill_amt6\",\n",
    "    \"Pay_amt1\",\n",
    "    \"Pay_amt2\",\n",
    "    \"Pay_amt3\",\n",
    "    \"Pay_amt4\",\n",
    "    \"Pay_amt5\",\n",
    "    \"Pay_amt6\",\n",
    "]\n",
    "\n",
    "# Setup preprocessing and model pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[(\"robust_scaler\", RobustScaler(), features_robust)],\n",
    "    remainder=\"passthrough\",\n",
    ")\n",
    "\n",
    "# Create the pipeline with preprocessing and the LightGBM classifier\n",
    "pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"classifier\", LGBMClassifier(**parameters))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0605d68d-d020-4c47-b9d9-7a69a17ce134",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/28 20:58:58 INFO mlflow.tracking.fluent: Experiment with name '/Shared/test-feature' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 5323, number of negative: 18677\n[LightGBM] [Info] Total Bins 3249\n[LightGBM] [Info] Number of data points in the train set: 24000, number of used features: 23\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.221792 -> initscore=-1.255256\n[LightGBM] [Info] Start training from score -1.255256\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC: 0.6558141322330031\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/mlflow/types/schema.py:679: FutureWarning: `optional` is deprecated and will be removed in a future version of MLflow. Use `required` instead.\n  warnings.warn(\n2025/03/28 20:59:09 INFO mlflow.tracking._tracking_service.client: üèÉ View run likeable-fox-568 at: https://adb-4478913909061743.3.azuredatabricks.net/ml/experiments/327870133523236/runs/b82a208266be4959bf9ceb1a86eac961.\n2025/03/28 20:59:09 INFO mlflow.tracking._tracking_service.client: üß™ View experiment at: https://adb-4478913909061743.3.azuredatabricks.net/ml/experiments/327870133523236.\n"
     ]
    }
   ],
   "source": [
    "# Set and start MLflow experiment\n",
    "mlflow.set_experiment(experiment_name=\"/Shared/test-feature\")\n",
    "\n",
    "with mlflow.start_run(tags={\"branch\": \"serving\"}) as run:\n",
    "    run_id = run.info.run_id\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    # Calculate and print metrics\n",
    "    auc_test = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    print(\"Test AUC:\", auc_test)\n",
    "\n",
    "    # Log model parameters, metrics, and model\n",
    "    mlflow.log_param(\"model_type\", \"LightGBM with preprocessing\")\n",
    "    mlflow.log_params(parameters)\n",
    "    mlflow.log_metric(\"AUC\", auc_test)\n",
    "\n",
    "    signature = infer_signature(model_input=X_train, model_output=y_pred)\n",
    "\n",
    "    # Log model with feature engineering\n",
    "    fe.log_model(\n",
    "        model=pipeline,\n",
    "        flavor=mlflow.sklearn,\n",
    "        artifact_path=\"lightgbm-pipeline-model-feature\",\n",
    "        training_set=training_set,\n",
    "        signature=signature,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "370eed18-1970-4bd3-903d-be59ef970dca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Id', 'Limit_bal', 'Sex', 'Education', 'Marriage', 'Age', 'Pay_0',\n       'Pay_2', 'Pay_3', 'Pay_4', 'Pay_5', 'Pay_6', 'Bill_amt1', 'Bill_amt2',\n       'Bill_amt3', 'Bill_amt4', 'Bill_amt5', 'Bill_amt6', 'Pay_amt1',\n       'Pay_amt2', 'Pay_amt3', 'Pay_amt4', 'Pay_amt5', 'Pay_amt6', 'Default'],\n      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(training_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dc800bc-4b3c-4d69-acd1-e90a9e7edf45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'test01.default.credit_model_feature'.\nCreated version '1' of model 'test01.default.credit_model_feature'.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<ModelVersion: aliases=[], creation_timestamp=1743195587817, current_stage=None, description='', last_updated_timestamp=1743195589475, name='test01.default.credit_model_feature', run_id='b82a208266be4959bf9ceb1a86eac961', run_link=None, source='dbfs:/databricks/mlflow-tracking/327870133523236/b82a208266be4959bf9ceb1a86eac961/artifacts/lightgbm-pipeline-model-feature', status='READY', status_message='', tags={}, user_id='ak36804n@pace.edu', version='1'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.register_model(\n",
    "    model_uri=f\"runs:/{run_id}/lightgbm-pipeline-model-feature\",\n",
    "    name=f\"{config.catalog_name}.{config.schema_name}.credit_model_feature\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70aee1da-a935-466b-b2fe-b8371799ab4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03. Feature_mlflow_experiment_notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}