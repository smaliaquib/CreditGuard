{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f4b238f-5cf8-4a3e-be99-c67dda6b13ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n  Obtaining dependency information for python-dotenv from https://files.pythonhosted.org/packages/1e/18/98a99ad95133c6a6e2005fe89faedf294a748bd5dc803008059409ac9b1e/python_dotenv-1.1.0-py3-none-any.whl.metadata\n  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\nDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\nInstalling collected packages: python-dotenv\nSuccessfully installed python-dotenv-1.1.0\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e98c819-68c6-4c47-91b7-c3e5f6ccd36a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from typing import Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from databricks.connect import DatabricksSession\n",
    "from dotenv import load_dotenv\n",
    "from loguru import logger\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_timestamp, to_utc_timestamp\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from utils import Config, Target, load_config, setup_logging\n",
    "from clean import DataCleaning\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38b9b0be-67d8-4112-abbc-91ad724ac3f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder.getOrCreate()\n",
    "spark = DatabricksSession.builder.getOrCreate()\n",
    "\n",
    "FILEPATH_DATABRICKS = os.environ[\"FILEPATH_DATABRICKS\"]\n",
    "PREPROCESSING_LOGS = os.environ[\"PREPROCESSING_LOGS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "360ce5a0-a98b-4b04-9c3b-165c6aa779eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    \"\"\"\n",
    "    A class for preprocessing credit default data, including scaling features.\n",
    "\n",
    "    Attributes:\n",
    "        data_cleaning (DataCleaning): An instance of the DataCleaning class used for data preprocessing.\n",
    "        cleaned_data (pd.DataFrame): The cleaned DataFrame after preprocessing.\n",
    "        features_robust (list): List of feature names for robust scaling.\n",
    "        X (pd.DataFrame): Features DataFrame after cleaning.\n",
    "        y (pd.Series): Target Series after cleaning.\n",
    "        preprocessor (ColumnTransformer): ColumnTransformer for scaling the features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filepath: str, config: Config, spark: SparkSession):\n",
    "        \"\"\"\n",
    "        Initializes the DataPreprocessor class.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): The path to the CSV file containing the data.\n",
    "            config (Config): The configuration model containing preprocessing settings.\n",
    "        \"\"\"\n",
    "        self.catalog_name = config.catalog_name\n",
    "        self.schema_name = config.schema_name\n",
    "        self.spark = spark\n",
    "\n",
    "        try:\n",
    "            # Initialize DataCleaning to preprocess data\n",
    "            logger.info(\"Initializing data cleaning process\")\n",
    "            self.data_cleaning = DataCleaning(filepath, config, spark)\n",
    "            self.cleaned_data = self.data_cleaning.preprocess_data()\n",
    "            logger.info(\"Data cleaning process completed\")\n",
    "\n",
    "            # Define robust features for scaling from config\n",
    "            self.features_robust = config.features.robust\n",
    "\n",
    "            # Define features and target\n",
    "            self.X = self.cleaned_data.drop(columns=[target.new_name for target in config.target])\n",
    "            self.y = self.cleaned_data[config.target[0].new_name]\n",
    "\n",
    "            # Set up the ColumnTransformer for scaling\n",
    "            logger.info(\"Setting up ColumnTransformer for scaling\")\n",
    "            self.preprocessor = ColumnTransformer(\n",
    "                transformers=[\n",
    "                    (\"robust_scaler\", RobustScaler(), self.features_robust)  # Apply RobustScaler to selected features\n",
    "                ],\n",
    "                remainder=\"passthrough\",  # Keep other columns unchanged\n",
    "            )\n",
    "        except KeyError as e:\n",
    "            logger.error(f\"KeyError encountered during initialization: {str(e)}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An error occurred during initialization: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def get_processed_data(self) -> Tuple:\n",
    "        \"\"\"\n",
    "        Retrieves the processed features, target, and preprocessor.\n",
    "\n",
    "        Returns:\n",
    "            Tuple: A tuple containing:\n",
    "                - pd.DataFrame: The features DataFrame.\n",
    "                - pd.Series: The target Series.\n",
    "                - ColumnTransformer: The preprocessor for scaling.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"Retrieving processed data and preprocessor\")\n",
    "            logger.info(f\"Feature columns in X: {self.X.columns.tolist()}\")\n",
    "\n",
    "            # Log shapes of processed data\n",
    "            logger.info(f\"Data preprocessing completed. Shape of X: {self.X.shape}, Shape of y: {self.y.shape}\")\n",
    "\n",
    "            return self.X, self.y, self.preprocessor\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An error occurred during data preprocessing: {str(e)}\")\n",
    "\n",
    "    def split_data(self, test_size=0.2, random_state=42) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Split the cleaned DataFrame into training and test sets.\"\"\"\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            self.X, self.y, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "        train_set = pd.concat([X_train, y_train], axis=1)\n",
    "        test_set = pd.concat([X_test, y_test], axis=1)\n",
    "        return train_set, test_set\n",
    "\n",
    "    def save_to_catalog(self, train_set: pd.DataFrame, test_set: pd.DataFrame, spark: SparkSession):\n",
    "        \"\"\"Save the train and test sets into Databricks tables.\"\"\"\n",
    "        train_set_with_timestamp = spark.createDataFrame(train_set).withColumn(\n",
    "            \"Update_timestamp_utc\", to_utc_timestamp(current_timestamp(), \"UTC\")\n",
    "        )\n",
    "\n",
    "        test_set_with_timestamp = spark.createDataFrame(test_set).withColumn(\n",
    "            \"Update_timestamp_utc\", to_utc_timestamp(current_timestamp(), \"UTC\")\n",
    "        )\n",
    "\n",
    "        train_set_with_timestamp.write.mode(\"overwrite\").saveAsTable(\n",
    "            f\"{self.catalog_name}.{self.schema_name}.train_set\"\n",
    "        )\n",
    "\n",
    "        test_set_with_timestamp.write.mode(\"overwrite\").saveAsTable(f\"{self.catalog_name}.{self.schema_name}.test_set\")\n",
    "\n",
    "        spark.sql(\n",
    "            f\"ALTER TABLE {self.catalog_name}.{self.schema_name}.train_set \"\n",
    "            \"SET TBLPROPERTIES (delta.enableChangeDataFeed = true);\"\n",
    "        )\n",
    "\n",
    "        spark.sql(\n",
    "            f\"ALTER TABLE {self.catalog_name}.{self.schema_name}.test_set \"\n",
    "            \"SET TBLPROPERTIES (delta.enableChangeDataFeed = true);\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "414997c6-db4d-4bc5-8e94-5145c9d649ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-03-28 20:44:32\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mutils\u001B[0m:\u001B[36mload_config\u001B[0m - \u001B[1mLoaded configuration from ../project_config.yml\u001B[0m\n\u001B[32m2025-03-28 20:44:32\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mcommand-327870133523214-3460126103\u001B[0m:\u001B[36m<module>\u001B[0m - \u001B[1mInitializing DataPreprocessor with config: catalog_name='test01' schema_name='default' pipeline_id='4be66e88-11e8-4988-9fa3-459b9b28a83c' parameters={'learning_rate': 0.05, 'random_state': 42, 'force_col_wise': True} ab_test={'learning_rate_a': 0.05, 'learning_rate_b': 0.1, 'force_col_wise': True} num_features=[NumFeature(name='ID', dtype='int64'), NumFeature(name='LIMIT_BAL', dtype='float64'), NumFeature(name='SEX', dtype='int64'), NumFeature(name='EDUCATION', dtype='int64'), NumFeature(name='MARRIAGE', dtype='int64'), NumFeature(name='AGE', dtype='int64'), NumFeature(name='PAY_0', dtype='int64'), NumFeature(name='PAY_2', dtype='int64'), NumFeature(name='PAY_3', dtype='int64'), NumFeature(name='PAY_4', dtype='int64'), NumFeature(name='PAY_5', dtype='int64'), NumFeature(name='PAY_6', dtype='int64'), NumFeature(name='BILL_AMT1', dtype='float64'), NumFeature(name='BILL_AMT2', dtype='float64'), NumFeature(name='BILL_AMT3', dtype='float64'), NumFeature(name='BILL_AMT4', dtype='float64'), NumFeature(name='BILL_AMT5', dtype='float64'), NumFeature(name='BILL_AMT6', dtype='float64'), NumFeature(name='PAY_AMT1', dtype='float64'), NumFeature(name='PAY_AMT2', dtype='float64'), NumFeature(name='PAY_AMT3', dtype='float64'), NumFeature(name='PAY_AMT4', dtype='float64'), NumFeature(name='PAY_AMT5', dtype='float64'), NumFeature(name='PAY_AMT6', dtype='float64')] target=[Target(name='default.payment.next.month', dtype='int64', new_name='Default')] features=Features(clean=['Id', 'Limit_bal', 'Sex', 'Education', 'Marriage', 'Age', 'Pay_0', 'Pay_2', 'Pay_3', 'Pay_4', 'Pay_5', 'Pay_6', 'Bill_amt1', 'Bill_amt2', 'Bill_amt3', 'Bill_amt4', 'Bill_amt5', 'Bill_amt6', 'Pay_amt1', 'Pay_amt2', 'Pay_amt3', 'Pay_amt4', 'Pay_amt5', 'Pay_amt6'], robust=['Limit_bal', 'Bill_amt1', 'Bill_amt2', 'Bill_amt3', 'Bill_amt4', 'Bill_amt5', 'Bill_amt6', 'Pay_amt1', 'Pay_amt2', 'Pay_amt3', 'Pay_amt4', 'Pay_amt5', 'Pay_amt6'])\u001B[0m\n\u001B[32m2025-03-28 20:44:32\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mcommand-327870133523213-2484366695\u001B[0m:\u001B[36m__init__\u001B[0m - \u001B[1mInitializing data cleaning process\u001B[0m\n\u001B[32m2025-03-28 20:44:32\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mclean\u001B[0m:\u001B[36m_load_data\u001B[0m - \u001B[1mLoading data from dbfs:/Volumes/test01/default/data/data.csv\u001B[0m\n\u001B[32m2025-03-28 20:44:35\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mclean\u001B[0m:\u001B[36mpreprocess_data\u001B[0m - \u001B[1mStarting data preprocessing\u001B[0m\n\u001B[32m2025-03-28 20:44:35\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mclean\u001B[0m:\u001B[36m_rename_and_capitalize_columns\u001B[0m - \u001B[1mRenamed and capitalized columns\u001B[0m\n\u001B[32m2025-03-28 20:44:35\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mclean\u001B[0m:\u001B[36m_apply_value_corrections\u001B[0m - \u001B[1mApplying value corrections for Education, Marriage, and Pay columns\u001B[0m\n\u001B[32m2025-03-28 20:44:35\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mclean\u001B[0m:\u001B[36mpreprocess_data\u001B[0m - \u001B[1mData cleaning completed successfully\u001B[0m\n\u001B[32m2025-03-28 20:44:35\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mclean\u001B[0m:\u001B[36mpreprocess_data\u001B[0m - \u001B[1mFinal data shape: (30000, 25)\u001B[0m\n\u001B[32m2025-03-28 20:44:35\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mclean\u001B[0m:\u001B[36mpreprocess_data\u001B[0m - \u001B[1mFinal columns: ['Id', 'Limit_bal', 'Sex', 'Education', 'Marriage', 'Age', 'Pay_0', 'Pay_2', 'Pay_3', 'Pay_4', 'Pay_5', 'Pay_6', 'Bill_amt1', 'Bill_amt2', 'Bill_amt3', 'Bill_amt4', 'Bill_amt5', 'Bill_amt6', 'Pay_amt1', 'Pay_amt2', 'Pay_amt3', 'Pay_amt4', 'Pay_amt5', 'Pay_amt6', 'Default']\u001B[0m\n\u001B[32m2025-03-28 20:44:35\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mclean\u001B[0m:\u001B[36mpreprocess_data\u001B[0m - \u001B[1mID column data type: object\u001B[0m\n\u001B[32m2025-03-28 20:44:35\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mclean\u001B[0m:\u001B[36mpreprocess_data\u001B[0m - \u001B[1mSample of cleaned data:\n  Id  Limit_bal  Sex  Education  Marriage  Age  Pay_0  Pay_2  Pay_3  Pay_4  Pay_5  Pay_6  Bill_amt1  Bill_amt2  Bill_amt3  Bill_amt4  Bill_amt5  Bill_amt6  Pay_amt1  Pay_amt2  Pay_amt3  Pay_amt4  Pay_amt5  Pay_amt6  Default\n0  1    20000.0    2          2         1   24      2      2      0      0      0      0     3913.0     3102.0      689.0        0.0        0.0        0.0       0.0     689.0       0.0       0.0       0.0       0.0        1\n1  2   120000.0    2          2         2   26      0      2      0      0      0      2     2682.0     1725.0     2682.0     3272.0     3455.0     3261.0       0.0    1000.0    1000.0    1000.0       0.0    2000.0        1\n2  3    90000.0    2          2         2   34      0      0      0      0      0      0    29239.0    14027.0    13559.0    14331.0    14948.0    15549.0    1518.0    1500.0    1000.0    1000.0    1000.0    5000.0        0\n3  4    50000.0    2          2         1   37      0      0      0      0      0      0    46990.0    48233.0    49291.0    28314.0    28959.0    29547.0    2000.0    2019.0    1200.0    1100.0    1069.0    1000.0        0\n4  5    50000.0    1          2         1   57      0      0      0      0      0      0     8617.0     5670.0    35835.0    20940.0    19146.0    19131.0    2000.0   36681.0   10000.0    9000.0     689.0     679.0        0\u001B[0m\n\u001B[32m2025-03-28 20:44:35\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mclean\u001B[0m:\u001B[36mpreprocess_data\u001B[0m - \u001B[1mData cleaning script completed successfully\u001B[0m\n\u001B[32m2025-03-28 20:44:35\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mcommand-327870133523213-2484366695\u001B[0m:\u001B[36m__init__\u001B[0m - \u001B[1mData cleaning process completed\u001B[0m\n\u001B[32m2025-03-28 20:44:35\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mcommand-327870133523213-2484366695\u001B[0m:\u001B[36m__init__\u001B[0m - \u001B[1mSetting up ColumnTransformer for scaling\u001B[0m\n\u001B[32m2025-03-28 20:44:35\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mcommand-327870133523213-2484366695\u001B[0m:\u001B[36mget_processed_data\u001B[0m - \u001B[1mRetrieving processed data and preprocessor\u001B[0m\n\u001B[32m2025-03-28 20:44:35\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mcommand-327870133523213-2484366695\u001B[0m:\u001B[36mget_processed_data\u001B[0m - \u001B[1mFeature columns in X: ['Id', 'Limit_bal', 'Sex', 'Education', 'Marriage', 'Age', 'Pay_0', 'Pay_2', 'Pay_3', 'Pay_4', 'Pay_5', 'Pay_6', 'Bill_amt1', 'Bill_amt2', 'Bill_amt3', 'Bill_amt4', 'Bill_amt5', 'Bill_amt6', 'Pay_amt1', 'Pay_amt2', 'Pay_amt3', 'Pay_amt4', 'Pay_amt5', 'Pay_amt6']\u001B[0m\n\u001B[32m2025-03-28 20:44:35\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mcommand-327870133523213-2484366695\u001B[0m:\u001B[36mget_processed_data\u001B[0m - \u001B[1mData preprocessing completed. Shape of X: (30000, 24), Shape of y: (30000,)\u001B[0m\n\u001B[32m2025-03-28 20:44:35\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mcommand-327870133523214-3460126103\u001B[0m:\u001B[36m<module>\u001B[0m - \u001B[1mData split completed. Train shape: (24000, 25), Test shape: (6000, 25)\u001B[0m\n\u001B[32m2025-03-28 20:45:10\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mcommand-327870133523214-3460126103\u001B[0m:\u001B[36m<module>\u001B[0m - \u001B[1mTrain and test sets saved to catalog successfully.\u001B[0m\n\u001B[32m2025-03-28 20:45:10\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mcommand-327870133523214-3460126103\u001B[0m:\u001B[36m<module>\u001B[0m - \u001B[1mFeature columns in X: ['Id', 'Limit_bal', 'Sex', 'Education', 'Marriage', 'Age', 'Pay_0', 'Pay_2', 'Pay_3', 'Pay_4', 'Pay_5', 'Pay_6', 'Bill_amt1', 'Bill_amt2', 'Bill_amt3', 'Bill_amt4', 'Bill_amt5', 'Bill_amt6', 'Pay_amt1', 'Pay_amt2', 'Pay_amt3', 'Pay_amt4', 'Pay_amt5', 'Pay_amt6']\u001B[0m\n\u001B[32m2025-03-28 20:45:10\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mcommand-327870133523214-3460126103\u001B[0m:\u001B[36m<module>\u001B[0m - \u001B[1mData preprocessing completed. Shape of X: (30000, 24), Shape of y: (30000,)\u001B[0m\n\u001B[32m2025-03-28 20:45:10\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mcommand-327870133523214-3460126103\u001B[0m:\u001B[36m<module>\u001B[0m - \u001B[1mDataPreprocessor script completed\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configure logger using setup_logging\n",
    "    setup_logging(PREPROCESSING_LOGS)  # Set up logging with the log file path\n",
    "\n",
    "    # Load configuration from YAML file\n",
    "    config = load_config(\"../project_config.yml\")  # Returns Config instance\n",
    "\n",
    "    # Test the DataPreprocessor class\n",
    "    try:\n",
    "        logger.info(f\"Initializing DataPreprocessor with config: {config}\")\n",
    "        preprocessor = DataPreprocessor(FILEPATH_DATABRICKS, config, spark=spark)\n",
    "        X, y, preprocessor_model = preprocessor.get_processed_data()\n",
    "\n",
    "        # Split data into training and test sets\n",
    "        train_set, test_set = preprocessor.split_data()\n",
    "        logger.info(f\"Data split completed. Train shape: {train_set.shape}, Test shape: {test_set.shape}\")\n",
    "\n",
    "        # Save train and test sets to the Databricks catalog\n",
    "        preprocessor.save_to_catalog(train_set, test_set, spark)\n",
    "        logger.info(\"Train and test sets saved to catalog successfully.\")\n",
    "\n",
    "        logger.info(f\"Feature columns in X: {X.columns.tolist()}\")\n",
    "\n",
    "        # Log shapes of processed data\n",
    "        logger.info(f\"Data preprocessing completed. Shape of X: {X.shape}, Shape of y: {y.shape}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during data preprocessing: {str(e)}\")\n",
    "\n",
    "    logger.info(\"DataPreprocessor script completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8f672b5-efa6-41f3-b56f-4584f476ae3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02. Data_Preprocessing_Spark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}