{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6b371f3-e9f7-43c6-9619-ed14d107b1b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n  Obtaining dependency information for python-dotenv from https://files.pythonhosted.org/packages/1e/18/98a99ad95133c6a6e2005fe89faedf294a748bd5dc803008059409ac9b1e/python_dotenv-1.1.0-py3-none-any.whl.metadata\n  Using cached python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\nUsing cached python_dotenv-1.1.0-py3-none-any.whl (20 kB)\nInstalling collected packages: python-dotenv\nSuccessfully installed python-dotenv-1.1.0\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc178996-8a14-4805-b893-24f050954a0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from databricks.connect import DatabricksSession\n",
    "from dotenv import load_dotenv\n",
    "from loguru import logger\n",
    "from pydantic import ValidationError\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# from credit_default.utils import Config, Target\n",
    "from utils import Config, Target, load_config, setup_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03b72e03-6544-4e30-953b-48bfde91a872",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9233bc7-8fdb-4f48-bd86-d87f590a1ee8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = DatabricksSession.builder.getOrCreate()\n",
    "\n",
    "FILEPATH_DATABRICKS = os.environ[\"FILEPATH_DATABRICKS\"]\n",
    "CLEANING_LOGS = os.environ[\"CLEANING_LOGS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62f6eff8-5c3c-44e9-a089-f46d73244ad1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class DataCleaning:\n",
    "    \"\"\"\n",
    "    A class for cleaning and preprocessing credit default data.\n",
    "\n",
    "    Attributes:\n",
    "        config (Config): Configuration model containing preprocessing settings\n",
    "        df (pd.DataFrame): DataFrame containing the data to be processed\n",
    "        target_config (Target): Configuration for target variable\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filepath: str, config: Config, spark: SparkSession):\n",
    "        \"\"\"\n",
    "        Initializes the DataCleaning class.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to the CSV file containing the data\n",
    "            config (Config): Configuration model containing preprocessing settings\n",
    "\n",
    "        Raises:\n",
    "            Exception: If data cleaning fails\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.spark = spark\n",
    "        self.df = self._load_data(filepath)\n",
    "        self._setup_target_config()\n",
    "\n",
    "    def _setup_target_config(self) -> None:\n",
    "        \"\"\"Sets up target configuration from config.\"\"\"\n",
    "        target_info = self.config.target[0]\n",
    "        self.target_config = Target(name=target_info.name, dtype=target_info.dtype, new_name=target_info.new_name)\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_data(filepath: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Loads and validates the input data.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to the CSV file\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Loaded DataFrame\n",
    "\n",
    "        Raises:\n",
    "            Exception: If data loading or validation fails\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Loading data from {filepath}\")\n",
    "\n",
    "            df = spark.read.csv(FILEPATH_DATABRICKS, header=True, inferSchema=True).toPandas()\n",
    "\n",
    "            if df.empty:\n",
    "                raise Exception(\"Loaded DataFrame is empty\")\n",
    "            return df\n",
    "        except pd.errors.EmptyDataError as e:\n",
    "            raise Exception(f\"Failed to load data: {str(e)}\") from e\n",
    "\n",
    "    def _validate_columns(self) -> None:\n",
    "        \"\"\"\n",
    "        Validates that required columns exist in the DataFrame.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If DataFrame validation fails\n",
    "        \"\"\"\n",
    "        columns_to_check = [feature.name for feature in self.config.num_features] + [self.target_config.name]\n",
    "        missing_columns = [col for col in columns_to_check if col not in self.df.columns]\n",
    "        if missing_columns:\n",
    "            raise Exception(f\"Missing required columns: {', '.join(missing_columns)}\")\n",
    "\n",
    "    def _validate_data_types(self) -> None:\n",
    "        \"\"\"Validates data types of key columns.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If DataFrame validation fails\n",
    "        \"\"\"\n",
    "        target_col = self.target_config.name\n",
    "        if not np.issubdtype(self.df[target_col].dtype, np.number):\n",
    "            raise Exception(f\"Target column '{target_col}' must be numeric\")\n",
    "\n",
    "    def preprocess_data(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Preprocesses the data by performing several cleaning steps.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The cleaned DataFrame after preprocessing.\n",
    "\n",
    "        Raises:\n",
    "            Exception: If preprocessing fails\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(\"Starting data preprocessing\")\n",
    "            self._rename_and_capitalize_columns()\n",
    "            self._apply_value_corrections()\n",
    "            self._validate_preprocessed_data()\n",
    "\n",
    "            logger.info(\"Data cleaning completed successfully\")\n",
    "            logger.info(f\"Final data shape: {self.df.shape}\")\n",
    "            logger.info(f\"Final columns: {self.df.columns.tolist()}\")\n",
    "            logger.info(f\"ID column data type: {self.df['Id'].dtype}\")\n",
    "            logger.info(f\"Sample of cleaned data:\\n{self.df.head().to_string()}\")\n",
    "            logger.info(\"Data cleaning script completed successfully\")\n",
    "\n",
    "            return self.df\n",
    "\n",
    "        except ValidationError as e:\n",
    "            logger.error(f\"Configuration validation error: {e}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _rename_and_capitalize_columns(self) -> None:\n",
    "        \"\"\"Renames and capitalizes key columns.\"\"\"\n",
    "        self.df.rename(columns={self.target_config.name: self.target_config.new_name}, inplace=True)\n",
    "        self.df.columns = [col.capitalize() if col else col for col in self.df.columns]\n",
    "        self.df[\"Id\"] = self.df[\"Id\"].astype(\"str\")\n",
    "        logger.info(\"Renamed and capitalized columns\")\n",
    "\n",
    "    def _convert_int_to_float(self) -> None:\n",
    "        \"\"\"Converts integer columns to float to avoid schema enforcement errors with nulls.\"\"\"\n",
    "        logger.info(\"Converting integer columns to float (due to spark warning)\")\n",
    "        for col in self.df.select_dtypes(include=\"integer\").columns:\n",
    "            self.df[col] = self.df[col].astype(float)\n",
    "\n",
    "    def _apply_value_corrections(self) -> None:\n",
    "        \"\"\"Corrects unknown values in specified columns.\"\"\"\n",
    "        logger.info(\"Applying value corrections for Education, Marriage, and Pay columns\")\n",
    "        corrections = getattr(\n",
    "            self.config,\n",
    "            \"value_corrections\",\n",
    "            {\n",
    "                \"Education\": {0: 4, 5: 4, 6: 4},\n",
    "                \"Marriage\": {0: 3},\n",
    "                \"Pay\": {-1: 0, -2: 0},\n",
    "            },\n",
    "        )\n",
    "        for col_prefix, replacement_dict in corrections.items():\n",
    "            columns = [col for col in self.df.columns if col.startswith(col_prefix)]\n",
    "            for col in columns:\n",
    "                self.df[col] = self.df[col].replace(replacement_dict)\n",
    "\n",
    "    def _validate_preprocessed_data(self) -> None:\n",
    "        \"\"\"Validates the preprocessed data before returning.\"\"\"\n",
    "        if self.df.empty:\n",
    "            raise Exception(\"Preprocessing resulted in an empty DataFrame\")\n",
    "        target_col = self.target_config.new_name\n",
    "        if target_col not in self.df.columns:\n",
    "            raise Exception(f\"Target column '{target_col}' missing after preprocessing\")\n",
    "        if self.df.isnull().any().any():\n",
    "            raise Exception(\"Unexpected null values found after preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b6ec3e9-6549-45d3-89ba-942a430c50a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-03-28 22:12:14\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mutils\u001B[0m:\u001B[36mload_config\u001B[0m - \u001B[1mLoaded configuration from ../project_config.yml\u001B[0m\n\u001B[32m2025-03-28 22:12:14\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mcommand-327870133523197-3617429818\u001B[0m:\u001B[36m<module>\u001B[0m - \u001B[1mLoaded configuration from ../../project_config.yml\u001B[0m\n\u001B[32m2025-03-28 22:12:14\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mcommand-327870133523196-2066155600\u001B[0m:\u001B[36m_load_data\u001B[0m - \u001B[1mLoading data from dbfs:/Volumes/test01/default/data/data.csv\u001B[0m\n\u001B[32m2025-03-28 22:12:17\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mcommand-327870133523196-2066155600\u001B[0m:\u001B[36mpreprocess_data\u001B[0m - \u001B[1mStarting data preprocessing\u001B[0m\n\u001B[32m2025-03-28 22:12:17\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mcommand-327870133523196-2066155600\u001B[0m:\u001B[36m_rename_and_capitalize_columns\u001B[0m - \u001B[1mRenamed and capitalized columns\u001B[0m\n\u001B[32m2025-03-28 22:12:17\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mcommand-327870133523196-2066155600\u001B[0m:\u001B[36m_apply_value_corrections\u001B[0m - \u001B[1mApplying value corrections for Education, Marriage, and Pay columns\u001B[0m\n\u001B[32m2025-03-28 22:12:17\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mcommand-327870133523196-2066155600\u001B[0m:\u001B[36mpreprocess_data\u001B[0m - \u001B[1mData cleaning completed successfully\u001B[0m\n\u001B[32m2025-03-28 22:12:17\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mcommand-327870133523196-2066155600\u001B[0m:\u001B[36mpreprocess_data\u001B[0m - \u001B[1mFinal data shape: (30000, 25)\u001B[0m\n\u001B[32m2025-03-28 22:12:17\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mcommand-327870133523196-2066155600\u001B[0m:\u001B[36mpreprocess_data\u001B[0m - \u001B[1mFinal columns: ['Id', 'Limit_bal', 'Sex', 'Education', 'Marriage', 'Age', 'Pay_0', 'Pay_2', 'Pay_3', 'Pay_4', 'Pay_5', 'Pay_6', 'Bill_amt1', 'Bill_amt2', 'Bill_amt3', 'Bill_amt4', 'Bill_amt5', 'Bill_amt6', 'Pay_amt1', 'Pay_amt2', 'Pay_amt3', 'Pay_amt4', 'Pay_amt5', 'Pay_amt6', 'Default']\u001B[0m\n\u001B[32m2025-03-28 22:12:17\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mcommand-327870133523196-2066155600\u001B[0m:\u001B[36mpreprocess_data\u001B[0m - \u001B[1mID column data type: object\u001B[0m\n\u001B[32m2025-03-28 22:12:17\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mcommand-327870133523196-2066155600\u001B[0m:\u001B[36mpreprocess_data\u001B[0m - \u001B[1mSample of cleaned data:\n  Id  Limit_bal  Sex  Education  Marriage  Age  Pay_0  Pay_2  Pay_3  Pay_4  Pay_5  Pay_6  Bill_amt1  Bill_amt2  Bill_amt3  Bill_amt4  Bill_amt5  Bill_amt6  Pay_amt1  Pay_amt2  Pay_amt3  Pay_amt4  Pay_amt5  Pay_amt6  Default\n0  1    20000.0    2          2         1   24      2      2      0      0      0      0     3913.0     3102.0      689.0        0.0        0.0        0.0       0.0     689.0       0.0       0.0       0.0       0.0        1\n1  2   120000.0    2          2         2   26      0      2      0      0      0      2     2682.0     1725.0     2682.0     3272.0     3455.0     3261.0       0.0    1000.0    1000.0    1000.0       0.0    2000.0        1\n2  3    90000.0    2          2         2   34      0      0      0      0      0      0    29239.0    14027.0    13559.0    14331.0    14948.0    15549.0    1518.0    1500.0    1000.0    1000.0    1000.0    5000.0        0\n3  4    50000.0    2          2         1   37      0      0      0      0      0      0    46990.0    48233.0    49291.0    28314.0    28959.0    29547.0    2000.0    2019.0    1200.0    1100.0    1069.0    1000.0        0\n4  5    50000.0    1          2         1   57      0      0      0      0      0      0     8617.0     5670.0    35835.0    20940.0    19146.0    19131.0    2000.0   36681.0   10000.0    9000.0     689.0     679.0        0\u001B[0m\n\u001B[32m2025-03-28 22:12:17\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mcommand-327870133523196-2066155600\u001B[0m:\u001B[36mpreprocess_data\u001B[0m - \u001B[1mData cleaning script completed successfully\u001B[0m\n\u001B[32m2025-03-28 22:12:17\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mcommand-327870133523197-3617429818\u001B[0m:\u001B[36m<module>\u001B[0m - \u001B[1mData cleaning completed successfully\u001B[0m\n\u001B[32m2025-03-28 22:12:17\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mcommand-327870133523197-3617429818\u001B[0m:\u001B[36m<module>\u001B[0m - \u001B[1mFinal data shape: (30000, 25)\u001B[0m\n\u001B[32m2025-03-28 22:12:17\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mcommand-327870133523197-3617429818\u001B[0m:\u001B[36m<module>\u001B[0m - \u001B[1mFinal columns: ['Id', 'Limit_bal', 'Sex', 'Education', 'Marriage', 'Age', 'Pay_0', 'Pay_2', 'Pay_3', 'Pay_4', 'Pay_5', 'Pay_6', 'Bill_amt1', 'Bill_amt2', 'Bill_amt3', 'Bill_amt4', 'Bill_amt5', 'Bill_amt6', 'Pay_amt1', 'Pay_amt2', 'Pay_amt3', 'Pay_amt4', 'Pay_amt5', 'Pay_amt6', 'Default']\u001B[0m\n\u001B[32m2025-03-28 22:12:17\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mcommand-327870133523197-3617429818\u001B[0m:\u001B[36m<module>\u001B[0m - \u001B[1mSample of cleaned data:\n  Id  Limit_bal  Sex  Education  Marriage  Age  Pay_0  Pay_2  Pay_3  Pay_4  Pay_5  Pay_6  Bill_amt1  Bill_amt2  Bill_amt3  Bill_amt4  Bill_amt5  Bill_amt6  Pay_amt1  Pay_amt2  Pay_amt3  Pay_amt4  Pay_amt5  Pay_amt6  Default\n0  1    20000.0    2          2         1   24      2      2      0      0      0      0     3913.0     3102.0      689.0        0.0        0.0        0.0       0.0     689.0       0.0       0.0       0.0       0.0        1\n1  2   120000.0    2          2         2   26      0      2      0      0      0      2     2682.0     1725.0     2682.0     3272.0     3455.0     3261.0       0.0    1000.0    1000.0    1000.0       0.0    2000.0        1\n2  3    90000.0    2          2         2   34      0      0      0      0      0      0    29239.0    14027.0    13559.0    14331.0    14948.0    15549.0    1518.0    1500.0    1000.0    1000.0    1000.0    5000.0        0\n3  4    50000.0    2          2         1   37      0      0      0      0      0      0    46990.0    48233.0    49291.0    28314.0    28959.0    29547.0    2000.0    2019.0    1200.0    1100.0    1069.0    1000.0        0\n4  5    50000.0    1          2         1   57      0      0      0      0      0      0     8617.0     5670.0    35835.0    20940.0    19146.0    19131.0    2000.0   36681.0   10000.0    9000.0     689.0     679.0        0\u001B[0m\n\u001B[32m2025-03-28 22:12:17\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mcommand-327870133523197-3617429818\u001B[0m:\u001B[36m<module>\u001B[0m - \u001B[1mData cleaning script completed successfully\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Set up logging\n",
    "    setup_logging(CLEANING_LOGS)\n",
    "\n",
    "    try:\n",
    "        # Load configuration\n",
    "        config = load_config(\"../project_config.yml\")  # Returns Config instance\n",
    "        logger.info(f\"Loaded configuration from ../../project_config.yml\")\n",
    "        \n",
    "        # Create and run data cleaner\n",
    "        data_cleaner = DataCleaning(FILEPATH_DATABRICKS, config, spark)\n",
    "        cleaned_data = data_cleaner.preprocess_data()\n",
    "\n",
    "        # Log results\n",
    "        logger.info(\"Data cleaning completed successfully\")\n",
    "        logger.info(f\"Final data shape: {cleaned_data.shape}\")\n",
    "        logger.info(f\"Final columns: {cleaned_data.columns.tolist()}\")\n",
    "        logger.info(f\"Sample of cleaned data:\\n{cleaned_data.head().to_string()}\")\n",
    "\n",
    "    except ValidationError as e:\n",
    "        logger.error(f\"Configuration validation error: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "    logger.info(\"Data cleaning script completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "355587cd-0e38-4f9e-ba7d-fa1e4f5052a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01. Data_Cleaning_Spark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}