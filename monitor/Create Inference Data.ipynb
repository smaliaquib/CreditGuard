{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2406867-0866-4573-b5c0-fa3f01a98717",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-04-04 19:05:11.544\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36mcredit_default.utils\u001B[0m:\u001B[36mload_config\u001B[0m:\u001B[36m66\u001B[0m - \u001B[1mLoaded configuration from ../project_config.yml\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from credit_default.utils import load_config\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Load configuration\n",
    "config = load_config(\"../project_config.yml\")\n",
    "catalog_name = config.catalog_name\n",
    "schema_name = config.schema_name\n",
    "parameters = config.parameters\n",
    "target = config.target[0].new_name\n",
    "pipeline_id = config.pipeline_id\n",
    "\n",
    "# Load train/test set and convert to Pandas\n",
    "train_set = spark.table(f\"{catalog_name}.{schema_name}.train_set\").toPandas()\n",
    "\n",
    "test_set = spark.table(f\"{catalog_name}.{schema_name}.test_set\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c51ffaf-d07f-4cb8-88da-e50216edae55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 important features:\n      Feature  Importance\n5       Pay_0    0.097989\n4         Age    0.067260\n11  Bill_amt1    0.061741\n0   Limit_bal    0.061183\n"
     ]
    }
   ],
   "source": [
    "# Define features and target (adjust columns accordingly)\n",
    "X = train_set.drop(columns=[\"Id\", target, \"Update_timestamp_utc\"])\n",
    "y = train_set[target]\n",
    "\n",
    "# Train a Random Forest model\n",
    "model = RandomForestClassifier(random_state=parameters[\"random_state\"])\n",
    "model.fit(X, y)\n",
    "\n",
    "# Identify the most important features\n",
    "feature_importances = pd.DataFrame({\"Feature\": X.columns, \"Importance\": model.feature_importances_}).sort_values(\n",
    "    by=\"Importance\", ascending=False\n",
    ")\n",
    "\n",
    "print(\"Top 5 important features:\")\n",
    "print(feature_importances.head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b386ee2c-0573-413d-a7dc-90fc50ba1b32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get Existing IDs\n",
    "features_balanced = spark.table(f\"{catalog_name}.{schema_name}.features_balanced\").toPandas()\n",
    "existing_ids = set(int(id) for id in features_balanced[\"Id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "174edee4-df3c-4f2a-a6f0-c9a83b5b4d57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "37654"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(existing_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "664c3d9f-bd29-44c1-a4b7-01cf4dde8909",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define function to create synthetic data without random state\n",
    "# This will add some data drift in the above columns (if drift=True)\n",
    "\n",
    "def create_synthetic_data(df, drift=False, num_rows=100):\n",
    "    synthetic_data = pd.DataFrame()\n",
    "\n",
    "    for column in df.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df[column]) and column != \"Id\":\n",
    "            # Check if the column has a small set of discrete values\n",
    "            unique_values = df[column].unique()\n",
    "            if len(unique_values) <= 10:  # Assume discrete values if there are 10 or fewer unique values\n",
    "                # This includes all above columns except \"Age\"\n",
    "                synthetic_data[column] = np.random.choice(unique_values, num_rows)\n",
    "            elif column.startswith(\"Pay_amt\"):  # Ensure positive values for \"Pay_amt\" columns\n",
    "                mean, std = df[column].mean(), df[column].std()\n",
    "                synthetic_data[column] = np.abs(np.random.normal(mean, std, num_rows)).astype(int).astype(float)\n",
    "            else:\n",
    "                # This will add some data drift in the Bill_amt columns\n",
    "                mean, std = df[column].mean(), df[column].std()\n",
    "                synthetic_data[column] = np.round(np.random.normal(mean, std, num_rows)).astype(int).astype(float)\n",
    "\n",
    "        elif pd.api.types.is_datetime64_any_dtype(df[column]):\n",
    "            min_date, max_date = df[column].min(), df[column].max()\n",
    "            if min_date < max_date:\n",
    "                # Ensure the timestamp is between max_date and current time\n",
    "                current_time = pd.to_datetime(\"now\")\n",
    "                if max_date < current_time:\n",
    "                    timestamp_range_start = max_date.value\n",
    "                    timestamp_range_end = current_time.value\n",
    "                    synthetic_data[column] = pd.to_datetime(\n",
    "                        np.random.randint(timestamp_range_start, timestamp_range_end, num_rows)\n",
    "                    )\n",
    "                else:\n",
    "                    synthetic_data[column] = [max_date] * num_rows\n",
    "            else:\n",
    "                synthetic_data[column] = [min_date] * num_rows\n",
    "\n",
    "    new_ids = []\n",
    "    # The first synthetic Id must be one greater than the maximum existing Id of the whole dataframe (train + test). If no existing_ids, then starts from 1.\n",
    "    i = max(existing_ids) + 1 if existing_ids else 1\n",
    "\n",
    "    while len(new_ids) < num_rows:\n",
    "        if i not in existing_ids:\n",
    "            new_ids.append(str(i))  # Convert numeric ID to string\n",
    "        i += 1\n",
    "\n",
    "    synthetic_data[\"Id\"] = new_ids\n",
    "\n",
    "    # Move \"Id\" to the first position\n",
    "    columns = [\"Id\"] + [col for col in synthetic_data.columns if col != \"Id\"]\n",
    "    synthetic_data = synthetic_data[columns]\n",
    "\n",
    "    if drift:\n",
    "        # Skew the top features to introduce drift\n",
    "        top_features = [\"Limit_bal\", \"Age\", \"Pay_0\", \"Bill_amt1\"]  # Select top 4 features\n",
    "        for feature in top_features:\n",
    "            if feature in synthetic_data.columns:\n",
    "                synthetic_data[feature] = synthetic_data[feature] * 1.5\n",
    "\n",
    "    return synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd0635df-aeb3-47b9-9bec-7d6bcf075917",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id                              object\nLimit_bal                      float64\nSex                              int32\nEducation                        int32\nMarriage                         int32\nAge                            float64\nPay_0                            int32\nPay_2                            int32\nPay_3                            int32\nPay_4                            int32\nPay_5                            int32\nPay_6                            int32\nBill_amt1                      float64\nBill_amt2                      float64\nBill_amt3                      float64\nBill_amt4                      float64\nBill_amt5                      float64\nBill_amt6                      float64\nPay_amt1                       float64\nPay_amt2                       float64\nPay_amt3                       float64\nPay_amt4                       float64\nPay_amt5                       float64\nPay_amt6                       float64\nDefault                          int32\nUpdate_timestamp_utc    datetime64[ns]\ndtype: object\n        Id  Limit_bal  Sex  ...  Pay_amt6  Default          Update_timestamp_utc\n0    43655   170012.0    2  ...   12631.0        1 2025-04-04 18:12:29.124808894\n1    43656    89063.0    1  ...    6748.0        1 2025-04-04 18:09:47.309973874\n2    43657    86746.0    1  ...    5327.0        0 2025-04-04 17:47:31.834682892\n3    43658   -68078.0    2  ...    4163.0        0 2025-04-04 17:44:11.200917566\n4    43659   288818.0    1  ...   14994.0        0 2025-04-04 17:35:49.224636369\n..     ...        ...  ...  ...       ...      ...                           ...\n195  43850    69276.0    2  ...   12690.0        1 2025-04-04 18:43:46.589806627\n196  43851   295135.0    2  ...   19839.0        1 2025-04-04 18:18:03.886175347\n197  43852   199443.0    1  ...    1215.0        0 2025-04-04 18:17:32.583400012\n198  43853   416637.0    1  ...   12543.0        1 2025-04-04 18:34:19.502155088\n199  43854    -1165.0    2  ...   17115.0        0 2025-04-04 17:56:11.505272271\n\n[200 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create synthetic data normal\n",
    "combined_set = pd.concat([train_set, test_set], ignore_index=True)\n",
    "\n",
    "synthetic_data_normal = create_synthetic_data(combined_set, drift=False, num_rows=200)\n",
    "print(synthetic_data_normal.dtypes)\n",
    "print(synthetic_data_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7191db58-3dcd-4608-b5f4-e3d8ec624cb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 37654\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before: {len(existing_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2043bd9f-f510-4f75-852e-05e6ab7e6f8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Update existing_ids with the IDs from synthetic_data_normal\n",
    "existing_ids.update(int(id) for id in synthetic_data_normal[\"Id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00b18f1d-7d35-4cdf-a179-f18141b64b2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After: 37854\n"
     ]
    }
   ],
   "source": [
    "print(f\"After: {len(existing_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73edfc90-6dd7-426d-97f5-7c28ba561859",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id                              object\nLimit_bal                      float64\nSex                              int32\nEducation                        int32\nMarriage                         int32\nAge                            float64\nPay_0                            int32\nPay_2                            int32\nPay_3                            int32\nPay_4                            int32\nPay_5                            int32\nPay_6                            int32\nBill_amt1                      float64\nBill_amt2                      float64\nBill_amt3                      float64\nBill_amt4                      float64\nBill_amt5                      float64\nBill_amt6                      float64\nPay_amt1                       float64\nPay_amt2                       float64\nPay_amt3                       float64\nPay_amt4                       float64\nPay_amt5                       float64\nPay_amt6                       float64\nDefault                          int32\nUpdate_timestamp_utc    datetime64[ns]\ndtype: object\n        Id  Limit_bal  Sex  ...  Pay_amt6  Default          Update_timestamp_utc\n0    43855   -17761.5    2  ...   14396.0        0 2025-04-04 18:24:28.567346865\n1    43856   369226.5    2  ...    3399.0        0 2025-04-04 18:57:34.668923916\n2    43857   355174.5    2  ...   18105.0        1 2025-04-04 18:26:09.433146539\n3    43858    55693.5    1  ...    2434.0        0 2025-04-04 19:05:52.679457561\n4    43859   261336.0    2  ...   20509.0        1 2025-04-04 18:31:57.623138728\n..     ...        ...  ...  ...       ...      ...                           ...\n195  44050   227022.0    2  ...    4114.0        1 2025-04-04 17:49:58.634842829\n196  44051   429916.5    1  ...   14878.0        0 2025-04-04 17:28:12.205595928\n197  44052   269760.0    2  ...    9477.0        0 2025-04-04 18:32:08.821608067\n198  44053   320668.5    1  ...    6183.0        0 2025-04-04 17:44:49.244995603\n199  44054   162729.0    2  ...    4599.0        1 2025-04-04 17:46:34.148907269\n\n[200 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create synthetic data skewed\n",
    "synthetic_data_skewed = create_synthetic_data(combined_set, drift=True, num_rows=200)\n",
    "print(synthetic_data_normal.dtypes)\n",
    "print(synthetic_data_skewed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "392cc507-a708-4ad5-9da0-142677694e9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cast columns to match the schema of the Delta table\n",
    "columns_to_cast = [\"Sex\", \"Education\", \"Marriage\", \"Age\", \"Pay_0\", \"Pay_2\", \"Pay_3\", \"Pay_4\", \"Pay_5\", \"Pay_6\"]\n",
    "\n",
    "##  Write normal data to Delta Lake\n",
    "synthetic_normal_df = spark.createDataFrame(synthetic_data_normal)\n",
    "for column in columns_to_cast:\n",
    "    synthetic_normal_df = synthetic_normal_df.withColumn(column, F.col(column).cast(\"double\"))\n",
    "\n",
    "synthetic_normal_df.write.mode(\"append\").saveAsTable(f\"{catalog_name}.{schema_name}.inference_set_normal\")\n",
    "\n",
    "##  Write synthetic data to Delta Lake\n",
    "synthetic_skewed_df = spark.createDataFrame(synthetic_data_skewed)\n",
    "for column in columns_to_cast:\n",
    "    synthetic_skewed_df = synthetic_skewed_df.withColumn(column, F.col(column).cast(\"double\"))\n",
    "\n",
    "synthetic_skewed_df.write.mode(\"append\").saveAsTable(f\"{catalog_name}.{schema_name}.inference_set_skewed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea304097-d188-4cfa-b595-ffaabbd72cf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update offline table\n",
    "workspace = WorkspaceClient()\n",
    "\n",
    "columns = config.features.clean\n",
    "columns_str = \", \".join(columns)\n",
    "\n",
    "# Write normal into feature table; update online table\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO {catalog_name}.{schema_name}.features_balanced\n",
    "    SELECT {columns_str}\n",
    "    FROM {catalog_name}.{schema_name}.inference_set_normal\n",
    "\"\"\")\n",
    "\n",
    "# Write skewed into feature table; update online table\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO {catalog_name}.{schema_name}.features_balanced\n",
    "    SELECT {columns_str}\n",
    "    FROM {catalog_name}.{schema_name}.inference_set_skewed\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "426ea360-a63b-4c41-8244-74e0191d9ee3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline is in CREATED state.\nPipeline is waiting for resources.\nPipeline is in RUNNING state.\nPipeline is in RUNNING state.\n"
     ]
    }
   ],
   "source": [
    "# Update online table\n",
    "update_response = workspace.pipelines.start_update(pipeline_id=pipeline_id, full_refresh=False)\n",
    "\n",
    "while True:\n",
    "    update_info = workspace.pipelines.get_update(pipeline_id=pipeline_id, update_id=update_response.update_id)\n",
    "    state = update_info.update.state.value\n",
    "    if state == \"COMPLETED\":\n",
    "        break\n",
    "    elif state in [\"FAILED\", \"CANCELED\"]:\n",
    "        raise SystemError(\"Online table failed to update.\")\n",
    "    elif state == \"WAITING_FOR_RESOURCES\":\n",
    "        print(\"Pipeline is waiting for resources.\")\n",
    "    else:\n",
    "        print(f\"Pipeline is in {state} state.\")\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bec9087b-fa6a-4fbf-a075-42d2a06fb7c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Create Inference Data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}