{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26c3ce7a-3539-4a13-a220-207b28ff4243",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-04-04 20:53:18\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mcommand-7746108655932807-576299281\u001B[0m:\u001B[36m<module>\u001B[0m - \u001B[1mSpark session initialized.\u001B[0m\n\u001B[32m2025-04-04 20:53:18\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mcommand-7746108655932807-576299281\u001B[0m:\u001B[36m<module>\u001B[0m - \u001B[1mDatabricks workspace client initialized.\u001B[0m\n\u001B[32m2025-04-04 20:53:18\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mutils\u001B[0m:\u001B[36mload_config\u001B[0m - \u001B[1mLoaded configuration from ../project_config.yml\u001B[0m\n\u001B[32m2025-04-04 20:53:18\u001B[0m | \u001B[34m\u001B[1mDEBUG\u001B[0m | \u001B[36mcommand-7746108655932807-576299281\u001B[0m:\u001B[36m<module>\u001B[0m - \u001B[34m\u001B[1mCatalog: credit, Schema: default\u001B[0m\n\u001B[32m2025-04-04 20:53:18\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mcommand-7746108655932807-576299281\u001B[0m:\u001B[36m<module>\u001B[0m - \u001B[1mLoading inference table...\u001B[0m\n\u001B[32m2025-04-04 20:53:18\u001B[0m | \u001B[32m\u001B[1mSUCCESS\u001B[0m | \u001B[36mcommand-7746108655932807-576299281\u001B[0m:\u001B[36m<module>\u001B[0m - \u001B[32m\u001B[1mInference table loaded successfully.\u001B[0m\n\u001B[32m2025-04-04 20:53:18\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mcommand-7746108655932807-576299281\u001B[0m:\u001B[36m<module>\u001B[0m - \u001B[1mDefining schemas...\u001B[0m\n\u001B[32m2025-04-04 20:53:18\u001B[0m | \u001B[32m\u001B[1mSUCCESS\u001B[0m | \u001B[36mcommand-7746108655932807-576299281\u001B[0m:\u001B[36m<module>\u001B[0m - \u001B[32m\u001B[1mSchemas defined successfully.\u001B[0m\n\u001B[32m2025-04-04 20:53:18\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mcommand-7746108655932807-576299281\u001B[0m:\u001B[36m<module>\u001B[0m - \u001B[1mParsing request and response columns...\u001B[0m\n\u001B[32m2025-04-04 20:53:18\u001B[0m | \u001B[32m\u001B[1mSUCCESS\u001B[0m | \u001B[36mcommand-7746108655932807-576299281\u001B[0m:\u001B[36m<module>\u001B[0m - \u001B[32m\u001B[1mRequest and response parsed successfully.\u001B[0m\n\u001B[32m2025-04-04 20:53:18\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mcommand-7746108655932807-576299281\u001B[0m:\u001B[36m<module>\u001B[0m - \u001B[1mJoining data with train/test/inference sets...\u001B[0m\n\u001B[32m2025-04-04 20:53:19\u001B[0m | \u001B[32m\u001B[1mSUCCESS\u001B[0m | \u001B[36mcommand-7746108655932807-576299281\u001B[0m:\u001B[36m<module>\u001B[0m - \u001B[32m\u001B[1mData joined successfully.\u001B[0m\n\u001B[32m2025-04-04 20:53:19\u001B[0m | \u001B[1mINFO\u001B[0m | \u001B[36mcommand-7746108655932807-576299281\u001B[0m:\u001B[36m<module>\u001B[0m - \u001B[1mJoining with features and writing to model monitoring table...\u001B[0m\n\u001B[32m2025-04-04 20:53:19\u001B[0m | \u001B[31m\u001B[1mERROR\u001B[0m | \u001B[36mcommand-7746108655932807-576299281\u001B[0m:\u001B[36m<module>\u001B[0m - \u001B[31m\u001B[1mAn error occurred: [DELTA_FAILED_TO_MERGE_FIELDS] Failed to merge fields 'timestamp_ms' and 'timestamp_ms'\u001B[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "An exception has occurred, use %tb to see the full traceback.\n",
       "\n",
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-7746108655932807>, line 143\u001B[0m\n",
       "\u001B[1;32m    141\u001B[0m features_balanced \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mtable(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcatalog_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mschema_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.features_balanced\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m    142\u001B[0m df_final_with_features \u001B[38;5;241m=\u001B[39m df_final_with_status\u001B[38;5;241m.\u001B[39mjoin(features_balanced, on\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mId\u001B[39m\u001B[38;5;124m\"\u001B[39m, how\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mleft\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m--> 143\u001B[0m df_final_with_features\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msaveAsTable(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcatalog_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mschema_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.model_monitoring\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m    144\u001B[0m \u001B[38;5;66;03m# df_final_with_features.write.option(\"mergeSchema\", \"true\").mode(\"append\").saveAsTable(f\"{catalog_name}.{schema_name}.model_monitoring\")\u001B[39;00m\n",
       "\u001B[1;32m    146\u001B[0m logger\u001B[38;5;241m.\u001B[39msuccess(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mData written to model monitoring table successfully.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     50\u001B[0m     )\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1855\u001B[0m, in \u001B[0;36mDataFrameWriter.saveAsTable\u001B[0;34m(self, name, format, mode, partitionBy, **options)\u001B[0m\n",
       "\u001B[1;32m   1853\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mformat\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m   1854\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mformat\u001B[39m)\n",
       "\u001B[0;32m-> 1855\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msaveAsTable(name)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n",
       "\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n",
       "\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:261\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    257\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    258\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    259\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    260\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 261\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    262\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    263\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [DELTA_FAILED_TO_MERGE_FIELDS] Failed to merge fields 'timestamp_ms' and 'timestamp_ms'\n",
       "\n",
       "During handling of the above exception, another exception occurred:\n",
       "\n",
       "\u001B[0;31mSystemExit\u001B[0m\u001B[0;31m:\u001B[0m 1\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "SystemExit",
        "evalue": "1"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>SystemExit</span>: 1"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "An exception has occurred, use %tb to see the full traceback.\n",
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-7746108655932807>, line 143\u001B[0m\n\u001B[1;32m    141\u001B[0m features_balanced \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mtable(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcatalog_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mschema_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.features_balanced\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    142\u001B[0m df_final_with_features \u001B[38;5;241m=\u001B[39m df_final_with_status\u001B[38;5;241m.\u001B[39mjoin(features_balanced, on\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mId\u001B[39m\u001B[38;5;124m\"\u001B[39m, how\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mleft\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 143\u001B[0m df_final_with_features\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39msaveAsTable(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcatalog_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mschema_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.model_monitoring\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    144\u001B[0m \u001B[38;5;66;03m# df_final_with_features.write.option(\"mergeSchema\", \"true\").mode(\"append\").saveAsTable(f\"{catalog_name}.{schema_name}.model_monitoring\")\u001B[39;00m\n\u001B[1;32m    146\u001B[0m logger\u001B[38;5;241m.\u001B[39msuccess(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mData written to model monitoring table successfully.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     50\u001B[0m     )\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1855\u001B[0m, in \u001B[0;36mDataFrameWriter.saveAsTable\u001B[0;34m(self, name, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m   1853\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mformat\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1854\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mformat\u001B[39m)\n\u001B[0;32m-> 1855\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msaveAsTable(name)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:261\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    257\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    258\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    259\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    260\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 261\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    262\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    263\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [DELTA_FAILED_TO_MERGE_FIELDS] Failed to merge fields 'timestamp_ms' and 'timestamp_ms'",
        "\nDuring handling of the above exception, another exception occurred:\n",
        "\u001B[0;31mSystemExit\u001B[0m\u001B[0;31m:\u001B[0m 1\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from loguru import logger\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, DoubleType, StringType, StructField, StructType, TimestampType\n",
    "\n",
    "# from databricks.connect import DatabricksSession\n",
    "from utils import load_config, setup_logging\n",
    "\n",
    "# Set up logging\n",
    "setup_logging(log_file=\"\")\n",
    "\n",
    "\n",
    "try:\n",
    "    # Initialize Spark session\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    logger.info(\"Spark session initialized.\")\n",
    "\n",
    "    # Initialize Databricks workspace client\n",
    "    workspace = WorkspaceClient()\n",
    "    logger.info(\"Databricks workspace client initialized.\")\n",
    "\n",
    "    # Extract configuration details\n",
    "    config = load_config(\"../project_config.yml\")\n",
    "    catalog_name = config.catalog_name\n",
    "    schema_name = config.schema_name\n",
    "    target = config.target[0].new_name\n",
    "    logger.debug(f\"Catalog: {catalog_name}, Schema: {schema_name}\")\n",
    "\n",
    "    # Load inference table\n",
    "    logger.info(\"Loading inference table...\")\n",
    "    inf_table = spark.sql(f\"SELECT * FROM {catalog_name}.{schema_name}.`credit-default-model-serving-feature_payload`\")\n",
    "    logger.success(\"Inference table loaded successfully.\")\n",
    "\n",
    "    ## Dataframe records on payload table under response column\n",
    "    # {\"dataframe_records\": [{\"Id\": \"43565\", \"Limit_bal\": 198341.0, \"Sex\": 2.0,\n",
    "    # \"Education\": 2.0, \"Marriage\": 2.0, \"Age\": 26.0, \"Pay_0\": 2.0, \"Pay_2\": 1.0,\n",
    "    # \"Pay_3\": 6.0, \"Pay_4\": 4.0, \"Pay_5\": 8.0, \"Pay_6\": 6.0, \"Bill_amt1\": -44077.0,\n",
    "    # \"Bill_amt2\": 15797.0, \"Bill_amt3\": 66567.0, \"Bill_amt4\": 54582.0, \"Bill_amt5\": 79211.0,\n",
    "    # \"Bill_amt6\": 129060.0, \"Pay_amt1\": 13545.0, \"Pay_amt2\": 20476.0, \"Pay_amt3\": 8616.0,\n",
    "    # \"Pay_amt4\": 3590.0, \"Pay_amt5\": 22999.0, \"Pay_amt6\": 3605.0}]}\n",
    "    logger.info(\"Defining schemas...\")\n",
    "    request_schema = StructType(\n",
    "        [\n",
    "            StructField(\n",
    "                \"dataframe_records\",\n",
    "                ArrayType(\n",
    "                    StructType(\n",
    "                        [\n",
    "                            StructField(\"Id\", StringType(), True),\n",
    "                            StructField(\"Limit_bal\", DoubleType(), True),\n",
    "                            StructField(\"Sex\", DoubleType(), True),\n",
    "                            StructField(\"Education\", DoubleType(), True),\n",
    "                            StructField(\"Marriage\", DoubleType(), True),\n",
    "                            StructField(\"Age\", DoubleType(), True),\n",
    "                            StructField(\"Pay_0\", DoubleType(), True),\n",
    "                            StructField(\"Pay_2\", DoubleType(), True),\n",
    "                            StructField(\"Pay_3\", DoubleType(), True),\n",
    "                            StructField(\"Pay_4\", DoubleType(), True),\n",
    "                            StructField(\"Pay_5\", DoubleType(), True),\n",
    "                            StructField(\"Pay_6\", DoubleType(), True),\n",
    "                            StructField(\"Bill_amt1\", DoubleType(), True),\n",
    "                            StructField(\"Bill_amt2\", DoubleType(), True),\n",
    "                            StructField(\"Bill_amt3\", DoubleType(), True),\n",
    "                            StructField(\"Bill_amt4\", DoubleType(), True),\n",
    "                            StructField(\"Bill_amt5\", DoubleType(), True),\n",
    "                            StructField(\"Bill_amt6\", DoubleType(), True),\n",
    "                            StructField(\"Pay_amt1\", DoubleType(), True),\n",
    "                            StructField(\"Pay_amt2\", DoubleType(), True),\n",
    "                            StructField(\"Pay_amt3\", DoubleType(), True),\n",
    "                            StructField(\"Pay_amt4\", DoubleType(), True),\n",
    "                            StructField(\"Pay_amt5\", DoubleType(), True),\n",
    "                            StructField(\"Pay_amt6\", DoubleType(), True),\n",
    "                        ]\n",
    "                    )\n",
    "                ),\n",
    "                True,\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Standard Databricks schema for the response\n",
    "    response_schema = StructType(\n",
    "        [\n",
    "            StructField(\"predictions\", ArrayType(DoubleType()), True),\n",
    "            StructField(\n",
    "                \"databricks_output\",\n",
    "                StructType(\n",
    "                    [StructField(\"trace\", StringType(), True), StructField(\"databricks_request_id\", StringType(), True)]\n",
    "                ),\n",
    "                True,\n",
    "            ),\n",
    "        ]\n",
    "    )  \n",
    "    logger.success(\"Schemas defined successfully.\")\n",
    "\n",
    "    # Parse request and response\n",
    "    logger.info(\"Parsing request and response columns...\")\n",
    "    inf_table_parsed = inf_table.withColumn(\"parsed_request\", F.from_json(F.col(\"request\"), request_schema))\n",
    "\n",
    "    inf_table_parsed = inf_table_parsed.withColumn(\"parsed_response\", F.from_json(F.col(\"response\"), response_schema))\n",
    "\n",
    "    df_exploded = inf_table_parsed.withColumn(\"record\", F.explode(F.col(\"parsed_request.dataframe_records\")))\n",
    "\n",
    "    df_final = df_exploded.select(\n",
    "        F.from_unixtime(F.col(\"timestamp_ms\") / 1000).cast(\"timestamp\").alias(\"timestamp\"),\n",
    "        \"timestamp_ms\",\n",
    "        \"databricks_request_id\",\n",
    "        \"execution_time_ms\",\n",
    "        F.col(\"record.Id\").alias(\"Id\"),\n",
    "        F.col(\"parsed_response.predictions\")[0].alias(\"prediction\"),\n",
    "        F.lit(\"credit_model_feature\").alias(\"model_name\"),\n",
    "    )\n",
    "    logger.success(\"Request and response parsed successfully.\")\n",
    "\n",
    "    # Join data with train/test/inference sets\n",
    "    logger.info(\"Joining data with train/test/inference sets...\")\n",
    "    test_set = spark.table(f\"{catalog_name}.{schema_name}.train_set\")\n",
    "    inference_set_normal = spark.table(f\"{catalog_name}.{schema_name}.inference_set_normal\")\n",
    "    inference_set_skewed = spark.table(f\"{catalog_name}.{schema_name}.inference_set_skewed\")\n",
    "\n",
    "    inference_set = inference_set_normal.union(inference_set_skewed)\n",
    "\n",
    "    df_final_with_status = (\n",
    "        df_final.join(test_set.select(\"Id\", target), on=\"Id\", how=\"left\")\n",
    "        .withColumnRenamed(target, \"default_test\")\n",
    "        .join(inference_set.select(\"Id\", target), on=\"Id\", how=\"left\")\n",
    "        .withColumnRenamed(target, \"default_inference\")\n",
    "        .select(\"*\", F.coalesce(F.col(\"default_test\"), F.col(\"default_inference\")).alias(\"default\"))\n",
    "        .drop(\"default_test\", \"default_inference\")\n",
    "        .withColumn(\"default\", F.col(\"default\").cast(\"double\"))\n",
    "        .withColumn(\"prediction\", F.col(\"prediction\").cast(\"double\"))\n",
    "        # .dropna(subset=[\"default\", \"prediction\"])\n",
    "    )\n",
    "    logger.success(\"Data joined successfully.\")\n",
    "\n",
    "    # Join with features and write to model monitoring table\n",
    "    logger.info(\"Joining with features and writing to model monitoring table...\")\n",
    "    features_balanced = spark.table(f\"{catalog_name}.{schema_name}.features_balanced\")\n",
    "    df_final_with_features = df_final_with_status.join(features_balanced, on=\"Id\", how=\"left\")\n",
    "    df_final_with_features.write.mode(\"append\").saveAsTable(f\"{catalog_name}.{schema_name}.model_monitoring\")\n",
    "    # df_final_with_features.write.option(\"mergeSchema\", \"true\").mode(\"append\").saveAsTable(f\"{catalog_name}.{schema_name}.model_monitoring\")\n",
    "\n",
    "    logger.success(\"Data written to model monitoring table successfully.\")\n",
    "\n",
    "    # Run quality monitors\n",
    "    logger.info(\"Running quality monitors...\")\n",
    "    workspace.quality_monitors.run_refresh(table_name=f\"{catalog_name}.{schema_name}.model_monitoring\")\n",
    "    logger.success(\"Quality monitors refreshed successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"An error occurred: {e}\")\n",
    "    sys.exit(1)  # Exit with a failure code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c752589-fa09-4199-985a-0a7aa5b5f537",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Refresh Monitor",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}