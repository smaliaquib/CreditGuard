{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff8f1573-521a-4c72-a985-0815198b4471",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service.catalog import (\n",
    "    MonitorInferenceLog,\n",
    "    MonitorInferenceLogProblemType,\n",
    ")\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from utils import load_config\n",
    "from pyspark.sql.types import ArrayType, DoubleType, StringType, StructField, StructType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58793eb2-512c-46f3-bc7e-bbc699da99d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m2025-04-04 20:52:12.642\u001B[0m | \u001B[1mINFO    \u001B[0m | \u001B[36mutils\u001B[0m:\u001B[36mload_config\u001B[0m:\u001B[36m66\u001B[0m - \u001B[1mLoaded configuration from ../project_config.yml\u001B[0m\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = load_config(\"../project_config.yml\")\n",
    "catalog_name = config.catalog_name\n",
    "schema_name = config.schema_name\n",
    "\n",
    "# Create new monitoring table with complete schema\n",
    "monitoring_schema = StructType([\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"timestamp_ms\", DoubleType(), True),\n",
    "    StructField(\"databricks_request_id\", StringType(), True),\n",
    "    StructField(\"execution_time_ms\", DoubleType(), True),\n",
    "    StructField(\"Id\", StringType(), True),\n",
    "    StructField(\"prediction\", DoubleType(), True),\n",
    "    StructField(\"model_name\", StringType(), True),\n",
    "    StructField(\"default\", DoubleType(), True),\n",
    "    # Add all feature columns\n",
    "    StructField(\"Limit_bal\", DoubleType(), True),\n",
    "    StructField(\"Sex\", DoubleType(), True),\n",
    "    StructField(\"Education\", DoubleType(), True),\n",
    "    StructField(\"Marriage\", DoubleType(), True),\n",
    "    StructField(\"Age\", DoubleType(), True),\n",
    "    StructField(\"Pay_0\", DoubleType(), True),\n",
    "    StructField(\"Pay_2\", DoubleType(), True),\n",
    "    StructField(\"Pay_3\", DoubleType(), True),\n",
    "    StructField(\"Pay_4\", DoubleType(), True),\n",
    "    StructField(\"Pay_5\", DoubleType(), True),\n",
    "    StructField(\"Pay_6\", DoubleType(), True),\n",
    "    StructField(\"Bill_amt1\", DoubleType(), True),\n",
    "    StructField(\"Bill_amt2\", DoubleType(), True),\n",
    "    StructField(\"Bill_amt3\", DoubleType(), True),\n",
    "    StructField(\"Bill_amt4\", DoubleType(), True),\n",
    "    StructField(\"Bill_amt5\", DoubleType(), True),\n",
    "    StructField(\"Bill_amt6\", DoubleType(), True),\n",
    "    StructField(\"Pay_amt1\", DoubleType(), True),\n",
    "    StructField(\"Pay_amt2\", DoubleType(), True),\n",
    "    StructField(\"Pay_amt3\", DoubleType(), True),\n",
    "    StructField(\"Pay_amt4\", DoubleType(), True),\n",
    "    StructField(\"Pay_amt5\", DoubleType(), True),\n",
    "    StructField(\"Pay_amt6\", DoubleType(), True)\n",
    "])\n",
    "empty_monitoring_df = spark.createDataFrame([], monitoring_schema)\n",
    "empty_monitoring_df.write.format(\"delta\").saveAsTable(f\"{catalog_name}.{schema_name}.model_monitoring\")\n",
    "\n",
    "# Enable Change Data Feed for the table\n",
    "spark.sql(f\"ALTER TABLE {catalog_name}.{schema_name}.model_monitoring SET TBLPROPERTIES (delta.enableChangeDataFeed = true);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f8b61fc-8dc3-4504-ac84-2a8d6fb069ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "MonitorInfo(table_name='credit.default.model_monitoring', status=<MonitorInfoStatus.MONITOR_STATUS_PENDING: 'MONITOR_STATUS_PENDING'>, monitor_version=0, profile_metrics_table_name='credit.default.model_monitoring_profile_metrics', drift_metrics_table_name='credit.default.model_monitoring_drift_metrics', assets_dir='/Workspace/Shared/lakehouse_monitoring/credit.default.model_monitoring', baseline_table_name=None, custom_metrics=[], dashboard_id=None, data_classification_config=None, inference_log=MonitorInferenceLog(timestamp_col='timestamp', granularities=['30 minutes'], model_id_col='model_name', problem_type=<MonitorInferenceLogProblemType.PROBLEM_TYPE_CLASSIFICATION: 'PROBLEM_TYPE_CLASSIFICATION'>, prediction_col='prediction', label_col='default', prediction_proba_col=None), latest_monitor_failure_msg=None, notifications=None, output_schema_name='credit.default', schedule=None, slicing_exprs=None, snapshot=None, time_series=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workspace = WorkspaceClient()\n",
    "\n",
    "# Create quality monitor for the table with the right schema\n",
    "workspace.quality_monitors.create(\n",
    "    table_name=f\"{catalog_name}.{schema_name}.model_monitoring\",\n",
    "    assets_dir=f\"/Workspace/Shared/lakehouse_monitoring/{catalog_name}.{schema_name}.model_monitoring\",\n",
    "    output_schema_name=f\"{catalog_name}.{schema_name}\",\n",
    "    inference_log=MonitorInferenceLog(\n",
    "        problem_type=MonitorInferenceLogProblemType.PROBLEM_TYPE_CLASSIFICATION,\n",
    "        prediction_col=\"prediction\",\n",
    "        timestamp_col=\"timestamp\",\n",
    "        granularities=[\"30 minutes\"],\n",
    "        model_id_col=\"model_name\",\n",
    "        label_col=\"default\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99927024-0626-4efd-90f0-1bf941840408",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark.sql(f\"DROP TABLE IF EXISTS {catalog_name}.{schema_name}.model_monitoring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e365c59-c964-4258-b695-ceea4277ea98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNotFound\u001B[0m                                  Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-7746108655932792>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m## How to delete a monitor\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m workspace\u001B[38;5;241m.\u001B[39mquality_monitors\u001B[38;5;241m.\u001B[39mdelete(\n",
       "\u001B[1;32m      3\u001B[0m     table_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcatalog_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mschema_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.model_monitoring\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m      4\u001B[0m )\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/databricks/sdk/service/catalog.py:7887\u001B[0m, in \u001B[0;36mQualityMonitorsAPI.delete\u001B[0;34m(self, table_name)\u001B[0m\n",
       "\u001B[1;32m   7865\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Delete a table monitor.\u001B[39;00m\n",
       "\u001B[1;32m   7866\u001B[0m \u001B[38;5;124;03m\u001B[39;00m\n",
       "\u001B[1;32m   7867\u001B[0m \u001B[38;5;124;03mDeletes a monitor for the specified table.\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   7882\u001B[0m \u001B[38;5;124;03m\u001B[39;00m\n",
       "\u001B[1;32m   7883\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   7885\u001B[0m headers \u001B[38;5;241m=\u001B[39m {}\n",
       "\u001B[0;32m-> 7887\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_api\u001B[38;5;241m.\u001B[39mdo(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDELETE\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/api/2.1/unity-catalog/tables/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtable_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/monitor\u001B[39m\u001B[38;5;124m'\u001B[39m, headers\u001B[38;5;241m=\u001B[39mheaders)\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/databricks/sdk/core.py:155\u001B[0m, in \u001B[0;36mApiClient.do\u001B[0;34m(self, method, path, url, query, headers, body, raw, files, data, auth, response_headers)\u001B[0m\n",
       "\u001B[1;32m    151\u001B[0m headers[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mUser-Agent\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_agent_base\n",
       "\u001B[1;32m    152\u001B[0m retryable \u001B[38;5;241m=\u001B[39m retried(timeout\u001B[38;5;241m=\u001B[39mtimedelta(seconds\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retry_timeout_seconds),\n",
       "\u001B[1;32m    153\u001B[0m                     is_retryable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_is_retryable,\n",
       "\u001B[1;32m    154\u001B[0m                     clock\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cfg\u001B[38;5;241m.\u001B[39mclock)\n",
       "\u001B[0;32m--> 155\u001B[0m response \u001B[38;5;241m=\u001B[39m retryable(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_perform)(method,\n",
       "\u001B[1;32m    156\u001B[0m                                     url,\n",
       "\u001B[1;32m    157\u001B[0m                                     query\u001B[38;5;241m=\u001B[39mquery,\n",
       "\u001B[1;32m    158\u001B[0m                                     headers\u001B[38;5;241m=\u001B[39mheaders,\n",
       "\u001B[1;32m    159\u001B[0m                                     body\u001B[38;5;241m=\u001B[39mbody,\n",
       "\u001B[1;32m    160\u001B[0m                                     raw\u001B[38;5;241m=\u001B[39mraw,\n",
       "\u001B[1;32m    161\u001B[0m                                     files\u001B[38;5;241m=\u001B[39mfiles,\n",
       "\u001B[1;32m    162\u001B[0m                                     data\u001B[38;5;241m=\u001B[39mdata,\n",
       "\u001B[1;32m    163\u001B[0m                                     auth\u001B[38;5;241m=\u001B[39mauth)\n",
       "\u001B[1;32m    165\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m()\n",
       "\u001B[1;32m    166\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m header \u001B[38;5;129;01min\u001B[39;00m response_headers \u001B[38;5;28;01mif\u001B[39;00m response_headers \u001B[38;5;28;01melse\u001B[39;00m []:\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/databricks/sdk/retries.py:54\u001B[0m, in \u001B[0;36mretried.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     50\u001B[0m         retry_reason \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(err)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is allowed to retry\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
       "\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m retry_reason \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m     53\u001B[0m     \u001B[38;5;66;03m# raise if exception is not retryable\u001B[39;00m\n",
       "\u001B[0;32m---> 54\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m err\n",
       "\u001B[1;32m     56\u001B[0m logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRetrying: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mretry_reason\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (sleeping ~\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msleep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124ms)\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[1;32m     57\u001B[0m clock\u001B[38;5;241m.\u001B[39msleep(sleep \u001B[38;5;241m+\u001B[39m random())\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/databricks/sdk/retries.py:33\u001B[0m, in \u001B[0;36mretried.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m clock\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m<\u001B[39m deadline:\n",
       "\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 33\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m     34\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
       "\u001B[1;32m     35\u001B[0m         last_err \u001B[38;5;241m=\u001B[39m err\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/databricks/sdk/core.py:273\u001B[0m, in \u001B[0;36mApiClient._perform\u001B[0;34m(self, method, url, query, headers, body, raw, files, data, auth)\u001B[0m\n",
       "\u001B[1;32m    271\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_too_many_requests_or_unavailable:\n",
       "\u001B[1;32m    272\u001B[0m         error\u001B[38;5;241m.\u001B[39mretry_after_secs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_parse_retry_after(response)\n",
       "\u001B[0;32m--> 273\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    274\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
       "\n",
       "\u001B[0;31mNotFound\u001B[0m: Table credit.default.model_monitoring does not exist"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "NotFound",
        "evalue": "Table credit.default.model_monitoring does not exist"
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>NotFound</span>: Table credit.default.model_monitoring does not exist"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mNotFound\u001B[0m                                  Traceback (most recent call last)",
        "File \u001B[0;32m<command-7746108655932792>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m## How to delete a monitor\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m workspace\u001B[38;5;241m.\u001B[39mquality_monitors\u001B[38;5;241m.\u001B[39mdelete(\n\u001B[1;32m      3\u001B[0m     table_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcatalog_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mschema_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.model_monitoring\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      4\u001B[0m )\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/databricks/sdk/service/catalog.py:7887\u001B[0m, in \u001B[0;36mQualityMonitorsAPI.delete\u001B[0;34m(self, table_name)\u001B[0m\n\u001B[1;32m   7865\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Delete a table monitor.\u001B[39;00m\n\u001B[1;32m   7866\u001B[0m \u001B[38;5;124;03m\u001B[39;00m\n\u001B[1;32m   7867\u001B[0m \u001B[38;5;124;03mDeletes a monitor for the specified table.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   7882\u001B[0m \u001B[38;5;124;03m\u001B[39;00m\n\u001B[1;32m   7883\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   7885\u001B[0m headers \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m-> 7887\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_api\u001B[38;5;241m.\u001B[39mdo(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDELETE\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/api/2.1/unity-catalog/tables/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtable_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/monitor\u001B[39m\u001B[38;5;124m'\u001B[39m, headers\u001B[38;5;241m=\u001B[39mheaders)\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/databricks/sdk/core.py:155\u001B[0m, in \u001B[0;36mApiClient.do\u001B[0;34m(self, method, path, url, query, headers, body, raw, files, data, auth, response_headers)\u001B[0m\n\u001B[1;32m    151\u001B[0m headers[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mUser-Agent\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_agent_base\n\u001B[1;32m    152\u001B[0m retryable \u001B[38;5;241m=\u001B[39m retried(timeout\u001B[38;5;241m=\u001B[39mtimedelta(seconds\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retry_timeout_seconds),\n\u001B[1;32m    153\u001B[0m                     is_retryable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_is_retryable,\n\u001B[1;32m    154\u001B[0m                     clock\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cfg\u001B[38;5;241m.\u001B[39mclock)\n\u001B[0;32m--> 155\u001B[0m response \u001B[38;5;241m=\u001B[39m retryable(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_perform)(method,\n\u001B[1;32m    156\u001B[0m                                     url,\n\u001B[1;32m    157\u001B[0m                                     query\u001B[38;5;241m=\u001B[39mquery,\n\u001B[1;32m    158\u001B[0m                                     headers\u001B[38;5;241m=\u001B[39mheaders,\n\u001B[1;32m    159\u001B[0m                                     body\u001B[38;5;241m=\u001B[39mbody,\n\u001B[1;32m    160\u001B[0m                                     raw\u001B[38;5;241m=\u001B[39mraw,\n\u001B[1;32m    161\u001B[0m                                     files\u001B[38;5;241m=\u001B[39mfiles,\n\u001B[1;32m    162\u001B[0m                                     data\u001B[38;5;241m=\u001B[39mdata,\n\u001B[1;32m    163\u001B[0m                                     auth\u001B[38;5;241m=\u001B[39mauth)\n\u001B[1;32m    165\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m()\n\u001B[1;32m    166\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m header \u001B[38;5;129;01min\u001B[39;00m response_headers \u001B[38;5;28;01mif\u001B[39;00m response_headers \u001B[38;5;28;01melse\u001B[39;00m []:\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/databricks/sdk/retries.py:54\u001B[0m, in \u001B[0;36mretried.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     50\u001B[0m         retry_reason \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(err)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is allowed to retry\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m retry_reason \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     \u001B[38;5;66;03m# raise if exception is not retryable\u001B[39;00m\n\u001B[0;32m---> 54\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m err\n\u001B[1;32m     56\u001B[0m logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRetrying: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mretry_reason\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (sleeping ~\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msleep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124ms)\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     57\u001B[0m clock\u001B[38;5;241m.\u001B[39msleep(sleep \u001B[38;5;241m+\u001B[39m random())\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/databricks/sdk/retries.py:33\u001B[0m, in \u001B[0;36mretried.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m clock\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m<\u001B[39m deadline:\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 33\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m     34\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m     35\u001B[0m         last_err \u001B[38;5;241m=\u001B[39m err\n",
        "File \u001B[0;32m/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.11/site-packages/databricks/sdk/core.py:273\u001B[0m, in \u001B[0;36mApiClient._perform\u001B[0;34m(self, method, url, query, headers, body, raw, files, data, auth)\u001B[0m\n\u001B[1;32m    271\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_too_many_requests_or_unavailable:\n\u001B[1;32m    272\u001B[0m         error\u001B[38;5;241m.\u001B[39mretry_after_secs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_parse_retry_after(response)\n\u001B[0;32m--> 273\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    274\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m response\n",
        "\u001B[0;31mNotFound\u001B[0m: Table credit.default.model_monitoring does not exist"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## How to delete a monitor\n",
    "# workspace.quality_monitors.delete(\n",
    "#     table_name=f\"{catalog_name}.{schema_name}.model_monitoring\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b173dbf-ef16-4430-ab8a-0b1330bc8473",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Lakehouse Monitor",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}